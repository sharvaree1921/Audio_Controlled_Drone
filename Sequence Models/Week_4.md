### Transformers

#### Transformers Network Intuition

One of the very very important architectures in the NLP model.
Transformers Network allows you to compute all of these complicated complications in a parallel manner rather than sequentially from left to right as in RNN or LSTM.
RNN process one outtput at a time which contrasts with a CNN which takes in input a lot of pixels simultaneously(or in parallel). 

So, Attention+CNN is the attention. Self Attention and Multi-Headed Attention are its two types.

